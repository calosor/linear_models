{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99ec36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear model module\n",
    "#import modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from tabulate import tabulate\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "class ols():\n",
    "\n",
    "    #init variables\n",
    "\n",
    "    def __init__(self, dataset, dependent, regressors, cons = True, fixed_effects = [],\n",
    "    method = 'standard', cluster = []):\n",
    "        self.dataset = dataset\n",
    "        self.dependent = dependent\n",
    "        self.reggressors = regressors\n",
    "        self.cons = cons\n",
    "        self.fixed_effects = fixed_effects\n",
    "        self.method = method\n",
    "        self.cluster = cluster\n",
    "        self.cons_array = []\n",
    "        if self.cons is True:\n",
    "            # i need it to concatenate variables\n",
    "            self.cons_array = ['cons']\n",
    "        else:\n",
    "            self.cons_array = []\n",
    "        #get element zero from dependent vector\n",
    "        self.dep_var = self.dependent[0]\n",
    "\n",
    "\n",
    "    # prepare de dataset\n",
    "\n",
    "    def prep_data(self):\n",
    "        # cerate column for constant if constant is required in the model\n",
    "        if self.cons is True:\n",
    "            self.dataset[self.cons_array[0]] = np.ones(len(self.dataset))\n",
    "        # retrive keys for regressors\n",
    "        regressors = self.reggressors + self.cons_array\n",
    "        # create a sub sample of all regressors and dependent\n",
    "        if len(self.fixed_effects) != 0:\n",
    "            sub_sample = self.dataset[self.dependent + regressors +\n",
    "                                              self.fixed_effects].dropna()\n",
    "            fe_dummies = pd.get_dummies(sub_sample[self.fixed_effects], drop_first=True)\n",
    "            sub_sample = pd.concat([sub_sample.drop(self.fixed_effects, axis=1),\n",
    "                                    fe_dummies], axis=1)\n",
    "\n",
    "        else:\n",
    "            sub_sample = self.dataset[self.dependent + regressors +\n",
    "                                              self.fixed_effects].dropna()\n",
    "\n",
    "        return {'sample': self.dataset, 'sub_sample': sub_sample}\n",
    "\n",
    "    def coefficients(self):\n",
    "        X = self.prep_data().get('sub_sample').drop(self.dependent, axis = 1)\n",
    "        Y = self.prep_data().get('sub_sample')[self.dep_var]\n",
    "        return np.linalg.inv(X.T@X)@(X.T@Y).to_numpy().reshape((len(X.columns),1))\n",
    "\n",
    "    def fitted(self):\n",
    "        return self.prep_data().get('sub_sample').drop(self.dependent, axis = 1)\\\n",
    "               @self.coefficients()\n",
    "\n",
    "    def residuals(self):\n",
    "        Y = self.prep_data().get('sub_sample')[self.dep_var].to_numpy()\n",
    "        Y= Y.reshape((len(self.prep_data().get('sub_sample')),1))\n",
    "        return Y - self.fitted()\n",
    "\n",
    "    def AVAR(self):\n",
    "        X = self.prep_data().get('sub_sample').drop(self.dependent, axis = 1)\n",
    "        XX_inv = np.linalg.inv(X.to_numpy().T @ X.to_numpy())\n",
    "        if self.method == 'standard':\n",
    "            ssr = self.residuals().T@self.residuals()\n",
    "            dfg = 1/ (len(X) -len(X.columns) )\n",
    "\n",
    "            sigma_hat = ssr * dfg\n",
    "            return XX_inv* sigma_hat.values\n",
    "\n",
    "        elif self.method ==  'heter':\n",
    "            e_square = (self.residuals()**2).to_numpy().reshape((1, len(self.residuals())))[0]\n",
    "            e_diag = np.diag(e_square)\n",
    "            return (XX_inv @ (X.to_numpy().T @ e_diag @ X.to_numpy()) @ XX_inv)\n",
    "\n",
    "            #reference https://en.wikipedia.org/wiki/Heteroskedasticity-consistent_standard_errors\n",
    "            # compare with wooldridge examples\n",
    "\n",
    "        elif self.method == 'cluster':\n",
    "            #get the sample dataset\n",
    "            sample = self.prep_data().get('sample')\n",
    "            # be careful that cluster var is not repeated when it is also usead\n",
    "            # as fixed effect ???\n",
    "            keys = self.dependent + self.reggressors + self.cons_array +\\\n",
    "                 self.cluster + [i for i in self.fixed_effects\n",
    "            if i not in self.dependent + self.reggressors + self.cons_array + self.cluster ]\n",
    "\n",
    "            #select the subsample from keys and dropna (like stata)\n",
    "            sample  = sample[keys].dropna()\n",
    "            #generate dummies from fixed effects\n",
    "            dummies = pd.get_dummies(sample[self.fixed_effects], drop_first=True)\n",
    "            #nsert dummies in sample dataset\n",
    "            sample = pd.concat([sample, dummies], axis = 1)\n",
    "            #attach residuals column\n",
    "            sample['residuals'] = self.residuals()\n",
    "            #take unique clusters\n",
    "            clusters  = list(set(sample[self.cluster].values.reshape((1, len(sample[self.cluster])))[0]))\n",
    "            #set self cluster var as index to iterate across rows in the same cluster\n",
    "            sample = sample.set_index(self.cluster)\n",
    "            # for loop to iterate across clusters\n",
    "            shape_sigma = len(self.reggressors) + 1 + len(dummies.columns)\n",
    "            # initialize a zero sigma matrix. will be the sum of Bs for each cluster\n",
    "            sigma_matrix = np.zeros((shape_sigma,shape_sigma))\n",
    "            for c in (clusters):\n",
    "                # select the subsample for cluster c\n",
    "                c_sample = sample.loc[c]\n",
    "                # initialize eps as the vector of residuals of cluster c\n",
    "                eps_c = c_sample['residuals'].to_numpy().reshape((len(c_sample),1))\n",
    "                # inirialize the vector of regressors of cluster c\n",
    "                X_c = c_sample[[i for i in c_sample.columns\n",
    "                                if i not in self.dependent + self.fixed_effects + self.cluster\n",
    "                                +['residuals']]].values\n",
    "                # calculate Bs for each cluster and sum the zero matrix\n",
    "                sigma_matrix += X_c.T@eps_c@eps_c.T@X_c\n",
    "\n",
    "\n",
    "            return (XX_inv@sigma_matrix@XX_inv)\n",
    "\n",
    "\n",
    "    def standard_dev(self):\n",
    "        return (np.sqrt(np.diagonal(self.AVAR()))).reshape((len(self.coefficients()),1))\n",
    "\n",
    "    def t_stats(self):\n",
    "        return (np.round(self.coefficients() / self.standard_dev(),2))\n",
    "\n",
    "    def p_value(self):\n",
    "        tvec = self.t_stats()\n",
    "        dfree = len(self.prep_data().get('sub_sample').drop(self.dependent, axis = 1)) - 1\n",
    "        return np.round(scipy.stats.norm.sf(abs(tvec)) * 2,2)\n",
    "\n",
    "    def confidence(self):\n",
    "        betas = self.coefficients()\n",
    "        std = self.standard_dev()\n",
    "        low = betas - std*1.96\n",
    "        high = betas + std*1.96\n",
    "\n",
    "        return {'low': low, 'high': high}\n",
    "\n",
    "    def summary(self):\n",
    "        header = [self.dependent[0], 'coefficient', 'se', 't', 'p_value', 'low 95', 'high 95']\n",
    "        table = []\n",
    "        vars = self.reggressors + self.cons_array\n",
    "        def reshaping(array):\n",
    "            return array[0:len(vars)].reshape((1,len(vars)))[0]\n",
    "\n",
    "        vec = [vars, reshaping(self.coefficients()), reshaping(self.standard_dev()),\n",
    "        reshaping(self.t_stats()),reshaping(self.p_value()), reshaping(self.confidence().get('low')),\n",
    "        reshaping(self.confidence().get('high'))]\n",
    "        vec = list(map(list, zip(*vec)))\n",
    "        print('OLS Regression')\n",
    "\n",
    "        print('------------------------------------------------------------------------------------')\n",
    "        print(tabulate(vec, headers=header))\n",
    "        print('------------------------------------------------------------------------------------')\n",
    "        return ''\n",
    "\n",
    "\n",
    "class two_sls():\n",
    "\n",
    "    def __init__(self, dataset, dependent, regressors, endogenous, instruments,\n",
    "                 cons = True, fixed_effects = []):\n",
    "        self.dataset = dataset\n",
    "        self.dependent = dependent\n",
    "        self.regressors = regressors\n",
    "        self.endogenous = endogenous\n",
    "        self.instruments = instruments\n",
    "        self. cons = cons\n",
    "        if self.cons is True:\n",
    "            self.cons_arr = ['cons']\n",
    "        else:\n",
    "            self.cons_arr = []\n",
    "        self.fixed_effects = fixed_effects\n",
    "\n",
    "    def retrive_data(self):\n",
    "        data = self.dataset[self.dependent + self.regressors +\n",
    "                            self.endogenous + self.instruments].dropna()\n",
    "        if len(self.fixed_effects) == 0:\n",
    "            return data\n",
    "        else:\n",
    "            dummies = pd.get_dummies(self.fixed_effects, drop_first= True)\n",
    "            return pd.concat([data, dummies], axis = 1)\n",
    "\n",
    "\n",
    "    def first_stage(self):\n",
    "        # define list of regressors for first stage\n",
    "\n",
    "        fs_regressors = [i for i in self.retrive_data().columns.tolist() if\n",
    "                         i not in self.dependent + self.endogenous]\n",
    "\n",
    "        #initializing the linear regression for the first stage\n",
    "        betas_matrix = np.zeros((len(fs_regressors)+1\n",
    "                                 , 1))\n",
    "        fitted_matrix = np.zeros((len(self.retrive_data()),1))\n",
    "\n",
    "        for end in range(len(self.endogenous)):\n",
    "            fs_obj = ols(dataset=self.retrive_data(), regressors=fs_regressors, dependent=[self.endogenous[end]]\n",
    "                         , fixed_effects=self.fixed_effects)\n",
    "\n",
    "            betas_matrix= np.concatenate([betas_matrix, fs_obj.coefficients()], axis=1)\n",
    "            fitted_matrix =  np.concatenate([fitted_matrix, fs_obj.fitted()], axis=1)\n",
    "\n",
    "\n",
    "        betas_matrix = betas_matrix[:,1:]\n",
    "        fitted_matrix = fitted_matrix[:, 1:]\n",
    "        return {'betas' : betas_matrix, 'fitted' : fitted_matrix}\n",
    "\n",
    "\n",
    "    def second_stage(self):\n",
    "        # attach first stage fitted values in original datsaet\n",
    "        first_dataset = self.retrive_data()\n",
    "        for end in range(len(self.endogenous)):\n",
    "            first_dataset[f'1stage_{self.endogenous[end]}'] = self.first_stage().get('fitted')[:,end]\n",
    "\n",
    "\n",
    "        ss_xs = self.regressors + [i for i in first_dataset.columns.tolist() if\n",
    "                                       i not in self.dependent + self.endogenous\n",
    "                                       + self.instruments + self.regressors]\n",
    "\n",
    "\n",
    "        # initialize model for 2nd stage regression\n",
    "        model_ss = ols(first_dataset, dependent=self.dependent, regressors=ss_xs, fixed_effects=self.fixed_effects)\n",
    "        # computing beta coefficients\n",
    "        b_hat = model_ss.coefficients()\n",
    "        Z = first_dataset[self.regressors + self.instruments + self.cons_arr]\n",
    "        # get matrix with x variables\n",
    "        sub_sample_2nd = ols(first_dataset, dependent=self.dependent, regressors=self.regressors + self.endogenous,\n",
    "                             fixed_effects=self.fixed_effects).prep_data() \\\n",
    "            .get('sub_sample')\n",
    "        X_2nd = sub_sample_2nd.drop(self.dependent, axis=1)\n",
    "        Y = (sub_sample_2nd[self.dependent[0]].values)\n",
    "        Y = Y.reshape((len(Y), 1))\n",
    "\n",
    "        # computing fitted values for 2nd stage residuals\n",
    "        XB_hat = (X_2nd @ b_hat).values\n",
    "        XB_hat = XB_hat.reshape((len(XB_hat), 1))\n",
    "        # computing 2nd stage residuals\n",
    "        residual_2nd = Y - XB_hat\n",
    "        # computing sigma 2nd stage\n",
    "        sigma_hat = (residual_2nd.T @ residual_2nd) / (\n",
    "                len(residual_2nd) - len(self.regressors + self.endogenous))\n",
    "        # computing 2nd stage variance covariance matrix\n",
    "        X_hat_second = (Z.T @ X_2nd).values\n",
    "        X_hat_first = (np.linalg.inv(Z.T @ Z))\n",
    "        X_hat = Z @ (X_hat_first @ X_hat_second)\n",
    "        vars_matrix = sigma_hat * np.linalg.inv(X_hat.T @ X_hat)\n",
    "        std_var = np.sqrt(np.diagonal(vars_matrix)).reshape((len(b_hat), 1))\n",
    "        # computing t statistic\n",
    "        t = np.round(model_ss.coefficients() / std_var, 2)\n",
    "        # computing p value\n",
    "        dfree = len(model_ss.prep_data().get('sub_sample')) - 1\n",
    "        p_val = np.round(scipy.stats.t.sf(abs(t), df=dfree) * 2, 3)\n",
    "\n",
    "        # computing confidence bands\n",
    "        low = model_ss.coefficients() - (std_var * 1.96)\n",
    "        high = model_ss.coefficients() + (std_var * 1.96)\n",
    "\n",
    "        return {'beta': model_ss.coefficients(), 'std': std_var, 't': t, 'p': p_val, 'low': low, 'high': high,\n",
    "                'var_matrix': vars_matrix}\n",
    "\n",
    "    def summary(self):\n",
    "        print('2SLS Regression')\n",
    "        header = [self.dependent, 'coefficient', 'se', 't', 'p_value', 'low 95', 'high 95']\n",
    "        table = []\n",
    "        vars = self.regressors + self.endogenous + self.cons_arr\n",
    "\n",
    "        vec = [vars, self.second_stage().get('beta'), self.second_stage().get('std'), self.second_stage().get('t'),\n",
    "        self.second_stage().get('p'), self.second_stage().get('low'),  self.second_stage().get('high')]\n",
    "        vec = list(map(list, zip(*vec)))\n",
    "\n",
    "        print('-------------------------------------------------------------------------------')\n",
    "        print(tabulate(vec, headers=header))\n",
    "        print('-------------------------------------------------------------------------------')\n",
    "        print(f'Endogenous variable: {self.endogenous}')\n",
    "        print(f'Instruments: {self.regressors + self.instruments} ')\n",
    "        print('-------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f4b8d",
   "metadata": {},
   "source": [
    "# EXRECISE 5.4\n",
    "references: Wooldridge, J. M. (2010). Econometric Analysis of Cross Section and Panel Data. The MIT Press. http://www.jstor.org/stable/j.ctt5hhcfr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db97bc8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nearc2</th>\n",
       "      <th>nearc4</th>\n",
       "      <th>educ</th>\n",
       "      <th>age</th>\n",
       "      <th>fatheduc</th>\n",
       "      <th>motheduc</th>\n",
       "      <th>weight</th>\n",
       "      <th>momdad14</th>\n",
       "      <th>sinmom14</th>\n",
       "      <th>...</th>\n",
       "      <th>smsa66</th>\n",
       "      <th>wage</th>\n",
       "      <th>enroll</th>\n",
       "      <th>kww</th>\n",
       "      <th>iq</th>\n",
       "      <th>married</th>\n",
       "      <th>libcrd14</th>\n",
       "      <th>exper</th>\n",
       "      <th>lwage</th>\n",
       "      <th>expersq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158413.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>548</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>6.306275</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>380166.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>481</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>6.175867</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>367470.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>721</td>\n",
       "      <td>0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>6.580639</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>380166.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>5.521461</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>367470.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>729</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>6.591674</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>5218</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>82135.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5.814130</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>5219</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88765.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>481</td>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>6.175867</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>5220</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89271.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.214608</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008</th>\n",
       "      <td>5221</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110376.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>713</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>6.569481</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>5225</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81081.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>525</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>6.263398</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3010 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  nearc2  nearc4  educ  age  fatheduc  motheduc    weight  momdad14  \\\n",
       "0        2       0       0     7   29       NaN       NaN  158413.0         1   \n",
       "1        3       0       0    12   27       8.0       8.0  380166.0         1   \n",
       "2        4       0       0    12   34      14.0      12.0  367470.0         1   \n",
       "3        5       1       1    11   27      11.0      12.0  380166.0         1   \n",
       "4        6       1       1    12   34       8.0       7.0  367470.0         1   \n",
       "...    ...     ...     ...   ...  ...       ...       ...       ...       ...   \n",
       "3005  5218       0       1    12   25       8.0      12.0   82135.0         1   \n",
       "3006  5219       0       1    13   34       NaN       NaN   88765.0         1   \n",
       "3007  5220       0       1    12   24      11.0       NaN   89271.0         0   \n",
       "3008  5221       0       1    12   31       NaN       NaN  110376.0         1   \n",
       "3009  5225       0       1    13   26       NaN       NaN   81081.0         0   \n",
       "\n",
       "      sinmom14  ...  smsa66  wage  enroll   kww     iq  married  libcrd14  \\\n",
       "0            0  ...       1   548       0  15.0    NaN      1.0       0.0   \n",
       "1            0  ...       1   481       0  35.0   93.0      1.0       1.0   \n",
       "2            0  ...       1   721       0  42.0  103.0      1.0       1.0   \n",
       "3            0  ...       1   250       0  25.0   88.0      1.0       1.0   \n",
       "4            0  ...       1   729       0  34.0  108.0      1.0       0.0   \n",
       "...        ...  ...     ...   ...     ...   ...    ...      ...       ...   \n",
       "3005         0  ...       0   335       0  15.0    NaN      1.0       0.0   \n",
       "3006         0  ...       0   481       0  43.0    NaN      1.0       1.0   \n",
       "3007         0  ...       0   500       0  25.0  109.0      1.0       0.0   \n",
       "3008         0  ...       0   713       0  32.0  107.0      1.0       1.0   \n",
       "3009         0  ...       0   525       1  27.0    NaN      1.0       0.0   \n",
       "\n",
       "      exper     lwage  expersq  \n",
       "0        16  6.306275      256  \n",
       "1         9  6.175867       81  \n",
       "2        16  6.580639      256  \n",
       "3        10  5.521461      100  \n",
       "4        16  6.591674      256  \n",
       "...     ...       ...      ...  \n",
       "3005      7  5.814130       49  \n",
       "3006     15  6.175867      225  \n",
       "3007      6  6.214608       36  \n",
       "3008     13  6.569481      169  \n",
       "3009      7  6.263398       49  \n",
       "\n",
       "[3010 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD CARD DATASET (Wooldridge)\n",
    "import pandas as pd\n",
    "data = pd.read_stata('card.dta') \n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d6b0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Regression\n",
      "------------------------------------------------------------------------------------\n",
      "lwage      coefficient           se       t    p_value       low 95      high 95\n",
      "-------  -------------  -----------  ------  ---------  -----------  -----------\n",
      "educ       0.0746933    0.00349835    21.35       0      0.0678365    0.08155\n",
      "exper      0.084832     0.00662422    12.81       0      0.0718486    0.0978155\n",
      "expersq   -0.00228704   0.000316626   -7.22       0     -0.00290763  -0.00166645\n",
      "black     -0.199012     0.0182483    -10.91       0     -0.234779    -0.163246\n",
      "south     -0.147955     0.0259799     -5.69       0     -0.198876    -0.0970345\n",
      "smsa       0.136385     0.0201005      6.79       0      0.0969876    0.175781\n",
      "reg661    -0.11857      0.0388301     -3.05       0     -0.194677    -0.0424628\n",
      "reg662    -0.0222026    0.0282575     -0.79       0.43  -0.0775874    0.0331822\n",
      "reg663     0.0259703    0.0273644      0.95       0.34  -0.0276639    0.0796044\n",
      "reg664    -0.0634942    0.0356803     -1.78       0.08  -0.133428     0.00643918\n",
      "reg665     0.00945507   0.0361174      0.26       0.79  -0.061335     0.0802451\n",
      "reg666     0.0219476    0.0400984      0.55       0.58  -0.0566451    0.10054\n",
      "reg667    -0.000588746  0.0393793     -0.01       0.99  -0.0777722    0.0765947\n",
      "reg668    -0.175006     0.0463394     -3.78       0     -0.265831    -0.0841807\n",
      "smsa66     0.0262417    0.0194477      1.35       0.18  -0.0118757    0.0643592\n",
      "cons       4.73938      0.0715282     66.26       0      4.59918      4.87957\n",
      "------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform the ols regression\n",
    "\n",
    "ols_reg = ols(dataset = data, dependent=['lwage'], regressors=['educ','exper', 'expersq', 'black' ,'south', 'smsa', 'reg661'\n",
    "                                                              ,'reg662','reg663','reg664','reg665'\n",
    "                                                              ,'reg666','reg667','reg668','smsa66'])\n",
    "ols_reg.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82ecc89f",
   "metadata": {},
   "source": [
    "Output from STATA. Reference: Solution Manual Wooldridge, J. M. (2010). Econometric Analysis of Cross Section and Panel Data\n",
    "-----------------------------------------------------------------------------\n",
    "lwage |     Coef.    Std. Err.    t       P value [95% Conf. Interval\n",
    "----------------------------------------------------------------------------\n",
    "educ    | .0746933   | .0034983   |21.35   |0.000  .0678339 .0815527\n",
    "exper   | .084832    |.0066242    |12.81   |0.000  .0718435 .0978205\n",
    "expersq | -.002287   |.0003166    |-7.22   |0.000  -.0029079 -.0016662\n",
    "black   | -.1990123  |  .0182483  |-10.91  |0.000  -.2347927 -.1632318\n",
    "south   | -.147955   |.0259799    |-5.69   |0.000  -.1988952 -.0970148\n",
    "smsa    | .1363845   |.0201005    |6.79    |0.000  .0969724 .1757967\n",
    "reg661  | -.1185698  |.0388301    |-3.05   |0.002  -.194706 -.0424335\n",
    "reg662  | -.0222026  |.0282575    |-0.79   |0.432  -.0776088 .0332036\n",
    "reg663  | .0259703   |.0273644    |0.95    |0.343  -.0276846 .0796251\n",
    "reg664  | -.0634942  |.0356803    |-1.78   |0.075  -.1334546 .0064662\n",
    "reg665  | .0094551   |.0361174    |0.26    |0.794  -.0613623 .0802725\n",
    "reg666  | .0219476   |.0400984    |0.55    |0.584  -.0566755 .1005708\n",
    "reg667  | -.0005887  |.0393793    |-0.01   |0.988  -.077802 .0766245\n",
    "reg668  | -.1750058  |.0463394    |-3.78   |0.000  -.265866 -.0841456\n",
    "smsa66  | .0262417   |.0194477    |1.35    |0.177  -.0118905 .0643739\n",
    "_cons   | 4.739377   |.0715282    |66.26   |0.000   4.599127 4.879626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f1ef42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2SLS Regression\n",
      "-------------------------------------------------------------------------------\n",
      "['lwage']      coefficient           se      t    p_value       low 95      high 95\n",
      "-----------  -------------  -----------  -----  ---------  -----------  -----------\n",
      "exper           0.108271    0.0236546     4.58      0       0.061908     0.154634\n",
      "expersq        -0.00233494  0.000333441  -7         0      -0.00298848  -0.00168139\n",
      "black          -0.146776    0.0538909    -2.72      0.007  -0.252402    -0.0411497\n",
      "south          -0.144672    0.0272801    -5.3       0      -0.19814     -0.0912026\n",
      "smsa            0.111808    0.0316567     3.53      0       0.0497612    0.173855\n",
      "reg661         -0.107814    0.0418067    -2.58      0.01   -0.189755    -0.0258731\n",
      "reg662         -0.00704645  0.0329018    -0.21      0.834  -0.0715339    0.057441\n",
      "reg663          0.0404445   0.0317753     1.27      0.204  -0.021835     0.102724\n",
      "reg664         -0.0579172   0.0375996    -1.54      0.124  -0.131612     0.0157781\n",
      "reg665          0.0384577   0.0469309     0.82      0.412  -0.0535268    0.130442\n",
      "reg666          0.0550887   0.0526509     1.05      0.294  -0.0481071    0.158285\n",
      "reg667          0.026758    0.0488205     0.55      0.582  -0.0689303    0.122446\n",
      "reg668         -0.190891    0.0507029    -3.76      0      -0.290269    -0.0915136\n",
      "smsa66          0.0185311   0.021605      0.86      0.39   -0.0238147    0.0608769\n",
      "educ            0.131504    0.0549545     2.39      0.017   0.023793     0.239215\n",
      "cons            3.77397     0.934791      4.04      0       1.94177      5.60616\n",
      "-------------------------------------------------------------------------------\n",
      "Endogenous variable: ['educ']\n",
      "Instruments: ['exper', 'expersq', 'black', 'south', 'smsa', 'reg661', 'reg662', 'reg663', 'reg664', 'reg665', 'reg666', 'reg667', 'reg668', 'smsa66', 'nearc4'] \n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform 2sls regression\n",
    "\n",
    "twosls_reg = two_sls(dataset = data, dependent=['lwage'], regressors=['exper', 'expersq', 'black' ,'south', 'smsa', 'reg661'\n",
    "                                                              ,'reg662','reg663','reg664','reg665'\n",
    "                                                              ,'reg666','reg667','reg668','smsa66'],endogenous=['educ'],instruments=['nearc4'] \n",
    "                    )\n",
    "\n",
    "twosls_reg.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce740c70",
   "metadata": {},
   "source": [
    "OUTPUT FROM STATA RESULTS\n",
    "-----------------------------------------------------------------------------\n",
    "lwage | Coef. Std. Err. t P|t| [95% Conf. Interval\n",
    "----------------------------------------------------------------------------\n",
    "educ | .1315038 .0549637 2.39 0.017 .0237335 .2392742\n",
    "exper | .1082711 .0236586 4.58 0.000 .0618824 .1546598\n",
    "expersq | -.0023349 .0003335 -7.00 0.000 -.0029888 -.001681\n",
    "black | -.1467757 .0538999 -2.72 0.007 -.2524603 -.0410912\n",
    "south | -.1446715 .0272846 -5.30 0.000 -.19817 -.091173\n",
    "smsa | .1118083 .031662 3.53 0.000 .0497269 .1738898\n",
    "reg661 | -.1078142 .0418137 -2.58 0.010 -.1898007 -.0258278\n",
    "reg662 | -.0070465 .0329073 -0.21 0.830 -.0715696 .0574767\n",
    "reg663 | .0404445 .0317806 1.27 0.203 -.0218694 .1027585\n",
    "reg664 | -.0579172 .0376059 -1.54 0.124 -.1316532 .0158189\n",
    "reg665 | .0384577 .0469387 0.82 0.413 -.0535777 .130493\n",
    "reg666 | .0550887 .0526597 1.05 0.296 -.0481642 .1583416\n",
    "reg667 | .026758 .0488287 0.55 0.584 -.0689832 .1224992\n",
    "reg668 | -.1908912 .0507113 -3.76 0.000 -.2903238 -.0914586\n",
    "smsa66 | .0185311 .0216086 0.86 0.391 -.0238381 .0609003\n",
    "_cons | 3.773965 .934947 4.04 0.000 1.940762 5.607169\n",
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c4528c",
   "metadata": {},
   "source": [
    "# Exercise 5.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2964214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nls80 = pd.read_stata('nls80.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cd8ac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2SLS Regression\n",
      "-------------------------------------------------------------------------------\n",
      "['lwage']      coefficient          se      t    p_value        low 95     high 95\n",
      "-----------  -------------  ----------  -----  ---------  ------------  ----------\n",
      "exper           0.0313987   0.0122452    2.56      0.011   0.00739819   0.0553992\n",
      "tenure          0.00704757  0.00336934   2.09      0.037   0.000443671  0.0136515\n",
      "married         0.213337    0.053491     3.99      0       0.108494     0.318179\n",
      "south          -0.0941667   0.0506034   -1.86      0.063  -0.193349     0.00501605\n",
      "urban           0.168072    0.0384068    4.38      0       0.0927947    0.243349\n",
      "black          -0.234571    0.224599    -1.04      0.299  -0.674786     0.205643\n",
      "educ            0.16469     0.113187     1.46      0.145  -0.0571553    0.386536\n",
      "iq             -0.0102736   0.0199983   -0.51      0.61   -0.0494704    0.0289231\n",
      "cons            4.93296     0.486671    10.14      0       3.97909      5.88684\n",
      "-------------------------------------------------------------------------------\n",
      "Endogenous variable: ['educ', 'iq']\n",
      "Instruments: ['exper', 'tenure', 'married', 'south', 'urban', 'black', 'kww', 'meduc', 'feduc', 'sibs'] \n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twosls_reg = two_sls(dataset = nls80, dependent=['lwage'], regressors=['exper', 'tenure', 'married' ,'south', 'urban', 'black']\n",
    "                                                              ,endogenous=['educ', 'iq'],instruments=['kww','meduc', 'feduc', 'sibs'] \n",
    "                    )\n",
    "\n",
    "twosls_reg.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5af3a78",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "lwage | Coef. Std. Err. t P|t| [95% Conf. Interval\n",
    "----------------------------------------------------------------------------\n",
    "educ | .1646904 .1132659 1.45 0.146 -.0576843 .3870651\n",
    "iq | -.0102736 .0200124 -0.51 0.608 -.0495638 .0290166\n",
    "exper | .0313987 .0122537 2.56 0.011 .007341 .0554564\n",
    "tenure | .0070476 .0033717 2.09 0.037 .0004279 .0136672\n",
    "married | .2133365 .0535285 3.99 0.000 .1082442 .3184289\n",
    "south | -.0941667 .0506389 -1.86 0.063 -.1935859 .0052525\n",
    "urban | .1680721 .0384337 4.37 0.000 .0926152 .2435289\n",
    "black | -.2345713 .2247568 -1.04 0.297 -.6758356 .2066929\n",
    "_cons | 4.932962 .4870124 10.13 0.000 3.976812 5.889112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6995592a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
